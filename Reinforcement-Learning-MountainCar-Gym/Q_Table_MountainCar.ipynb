{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsogIYe7GH1f"
   },
   "outputs": [],
   "source": [
    "%pip install gymnasium[classic-control]\n",
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BWXo2-bMGH1h"
   },
   "outputs": [],
   "source": [
    "class table_class:\n",
    "    def __init__(self, env, learning_rate, epsilon):\n",
    "\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.size = (env.observation_space.high - env.observation_space.low) / 100 # normalization\n",
    "        self.q_value_table = np.zeros((100,100,3))\n",
    "\n",
    "        # state [position speed]\n",
    "        # self.env.observation_space.low:[-1.2  -0.07]\n",
    "        # self.env.observation_space.high:[0.6  0.07]\n",
    "        # self.size:[0.036  0.0028]\n",
    "\n",
    "\n",
    "    def decresing_epsilon(self):\n",
    "        if self.epsilon > 0.5 :\n",
    "          self.epsilon = 90 * self.epsilon / 100\n",
    "        else :\n",
    "          self.epsilon = 98 * self.epsilon / 100\n",
    "\n",
    "        # to have always a bit of randomness\n",
    "        if self.epsilon < 0.001 :\n",
    "          self.epsilon = 0.005\n",
    "\n",
    "\n",
    "    def decresing_learning_rate(self):\n",
    "        self.learning_rate = 98 * self.learning_rate / 100\n",
    "\n",
    "\n",
    "    def action(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            return self.best_action(state[0],state[1])\n",
    "        else:\n",
    "             return np.random.choice(3)\n",
    "\n",
    "\n",
    "    def best_action(self, state_position, state_speed):\n",
    "        state_discrete_position = ((state_position - self.env.observation_space.low[0]) / self.size[0]).astype(np.int64)\n",
    "        state_discrete_speed = ((state_speed - self.env.observation_space.low[1]) / self.size[1]).astype(np.int64)\n",
    "        return np.argmax(self.q_value_table[state_discrete_position][state_discrete_speed]) # take the index of max q value (so the best action) # if all equal, return smallest index\n",
    "\n",
    "\n",
    "    def discretization(self, state, next_state):\n",
    "        state_discrete_position = ((state[0] - self.env.observation_space.low[0]) / self.size[0]).astype(np.int64)\n",
    "        state_discrete_speed = ((state[1] - self.env.observation_space.low[1]) / self.size[1]).astype(np.int64)\n",
    "\n",
    "        next_state_discrete_position = ((next_state[0] - self.env.observation_space.low[0]) / self.size[0]).astype(np.int64)\n",
    "        next_state_discrete_speed = ((next_state[1] - self.env.observation_space.low[1]) / self.size[1]).astype(np.int64)\n",
    "\n",
    "        return state_discrete_position, state_discrete_speed, next_state_discrete_position, next_state_discrete_speed\n",
    "\n",
    "\n",
    "    def update_table(self, state, next_state, action, reward, terminated):\n",
    "\n",
    "        state_discrete_position, state_discrete_speed, next_state_discrete_position, next_state_discrete_speed = self.discretization(state, next_state)\n",
    "\n",
    "        next_best_value = np.max(self.q_value_table[next_state_discrete_position][next_state_discrete_speed]) # take max q_value between the 3 actions\n",
    "\n",
    "        # 0.95 discount factor\n",
    "        self.q_value_table[state_discrete_position][state_discrete_speed][action] = ( (1 - self.learning_rate) * self.q_value_table[state_discrete_position][state_discrete_speed][action] + self.learning_rate * ( reward + 0.95 * next_best_value ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5Rm1R9lGH1j"
   },
   "outputs": [],
   "source": [
    "episodes = 150000\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "reward_single_episode = deque(maxlen=100)\n",
    "learning_rate = 0.1\n",
    "epsilon = 0.7\n",
    "win_episode = 0\n",
    "episode_result=[]\n",
    "reward_result=[]\n",
    "\n",
    "if not os.path.exists(\"table/\"):\n",
    "  os.makedirs(\"table/\")\n",
    "\n",
    "table = table_class(env, learning_rate, epsilon)\n",
    "\n",
    "for episode in range(1,episodes):\n",
    "    state, _ = env.reset()\n",
    "    step = 1\n",
    "    terminate, truncate = False, False\n",
    "\n",
    "    while not terminate and not truncate:\n",
    "\n",
    "        action = table.action(state)\n",
    "        next_state, reward, terminate, truncate, _ = env.step(action)\n",
    "        table.update_table(state, next_state, action, reward, terminate)\n",
    "\n",
    "        if terminate or truncate :\n",
    "\n",
    "            reward_single_episode.append(step * -1)\n",
    "            average_reward = sum(reward_single_episode) / len(reward_single_episode)\n",
    "\n",
    "            # if episode % 10 == 0: # for a best plotting\n",
    "            episode_result.append(episode)\n",
    "            reward_result.append(average_reward)\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode}, e {table.epsilon:.4f}, average_reward {average_reward:.2f}, position {state[0]:.4f}, speed {state[1]:.4f}, learning_rate {table.learning_rate:.4f}\")\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        step+=1\n",
    "\n",
    "    if (episode % 400 == 0 and episode > 0):\n",
    "        table.decresing_learning_rate()\n",
    "\n",
    "    if (episode % 200 == 0 and episode > 0):\n",
    "        table.decresing_epsilon()\n",
    "\n",
    "    if (episode % 1000 == 0 and episode > 0):\n",
    "\n",
    "        np.save(f'./table/{episode}-q.npy', table.q_value_table)\n",
    "\n",
    "        plt.plot(episode_result, reward_result)\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caTMGzGe2tBN"
   },
   "outputs": [],
   "source": [
    "env = gym.Env\n",
    "episodes = 1000\n",
    "load_file = \"table5\"\n",
    "average_reward, win_episode = [], 0\n",
    "\n",
    "#env = gym.make('MountainCar-v0', render_mode = \"human\")\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "table = table_class(env, learning_rate, epsilon)\n",
    "table.q_value_table = np.load(f'./table/{episode}.npy')\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    terminate, truncate, episode_reward = False, False, 0.0\n",
    "\n",
    "    while not terminate and not truncate:\n",
    "      action = table.best_action(state[0], state[1])\n",
    "\n",
    "      next_state, reward, terminate, truncate, _ = env.step(action)\n",
    "      episode_reward += reward\n",
    "      state = next_state\n",
    "\n",
    "      if next_state[0] >= 0.5:\n",
    "        win_episode += 1\n",
    "\n",
    "    average_reward.append(episode_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "      print(f\"Episode {episode}, Reward {episode_reward:.2f}\")\n",
    "\n",
    "\n",
    "mean = sum(average_reward) / len(average_reward)\n",
    "accuracy = win_episode / episodes\n",
    "\n",
    "print(f\"\\n\\nAverage Reward: {mean:.2f}, Accuracy {accuracy:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
