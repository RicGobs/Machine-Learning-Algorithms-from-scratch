{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-ic_huuZ28F"
   },
   "outputs": [],
   "source": [
    "%pip install gymnasium[classic-control]\n",
    "%pip install tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koZqhgpwZ28K"
   },
   "outputs": [],
   "source": [
    "class nnq:\n",
    "    def __init__(self, env, learning_rate, epsilon):\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.net = self.network()\n",
    "\n",
    "    def network(self):\n",
    "\n",
    "        model = Sequential() #input_shape = 2 -> position and speed\n",
    "\n",
    "        model.add(Dense(24, input_shape=env.observation_space.shape, activation='relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(36, activation='relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(3, activation='linear',kernel_initializer='he_uniform'))\n",
    "\n",
    "        opt = tf.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(self.learning_rate, decay_steps=300, decay_rate=0.96, staircase=True))\n",
    "        model.compile(optimizer=opt, loss='mse', metrics=[\"mse\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def best_action(self,state):\n",
    "        best_action_to_do = self.net(np.array([state]))\n",
    "        return np.argmax(best_action_to_do[0], axis=0) # use trained network to take best action\n",
    "\n",
    "\n",
    "    def action(self, state):\n",
    "        if random.random() > self.epsilon :\n",
    "            return self.best_action(state)\n",
    "        else:\n",
    "             return np.random.choice(3)\n",
    "\n",
    "    def replay_buffer(self, buffer, batch):\n",
    "\n",
    "        buffer_batch = random.sample(buffer, batch)\n",
    "\n",
    "        state = np.array([i[0] for i in buffer_batch])\n",
    "        action = np.array([i[1] for i in buffer_batch])\n",
    "        next_state = np.array([i[2] for i in buffer_batch])\n",
    "        reward = np.array([i[3] for i in buffer_batch])\n",
    "        terminate = np.array([i[4] for i in buffer_batch])\n",
    "\n",
    "        current_reward = self.net(state) #actual rewards\n",
    "        #target = np.zeros((state.shape[0],3))\n",
    "        target = np.copy(current_reward)\n",
    "\n",
    "        next_reward = self.net(next_state) #future rewards\n",
    "        max_next_reward = np.amax(next_reward, axis=1) #max reward in future reward (after three actions)\n",
    "\n",
    "        for e in range(state.shape[0]):\n",
    "            target[e][action[e]] = reward[e] + 0.99 * (1 - terminate[e]) * max_next_reward[e]\n",
    "\n",
    "        self.net.fit(x=state, y=target, epochs=1,verbose=0)\n",
    "        self.learning = self.net.optimizer.learning_rate.numpy()\n",
    "\n",
    "\n",
    "    def decresing_epsilon(self):\n",
    "        if self.epsilon > 0.3 :\n",
    "          self.epsilon = 95 * self.epsilon / 100\n",
    "        else :\n",
    "          self.epsilon = 995 * self.epsilon / 1000\n",
    "\n",
    "        # to have always a bit of randomness\n",
    "        if self.epsilon < 0.001 :\n",
    "          self.epsilon = 0.009\n",
    "\n",
    "\n",
    "    def save(self, episode):\n",
    "        self.net.save(f'./nn/nn-{episode}.h5')\n",
    "\n",
    "\n",
    "    def load(self, episode):\n",
    "        self.net = load_model(f'./nn/nn-{episode}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNqBB7BSZ28O"
   },
   "outputs": [],
   "source": [
    "episodes = 3500\n",
    "learning_rate = 0.01\n",
    "epsilon = 1.0\n",
    "batch = 64\n",
    "\n",
    "reward_single_episode = deque(maxlen=100) # my goal is terminate in 100 steps\n",
    "win_episode = 0\n",
    "episode_result=[]\n",
    "reward_result=[]\n",
    "\n",
    "if not os.path.exists(\"nn/\"):\n",
    "  os.makedirs(\"nn/\")\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "nnq_class = nnq(env, learning_rate, epsilon)\n",
    "\n",
    "#print(env.action_space)  ->  Discrete(3)\n",
    "#print(env.observation_space)  ->   Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
    "#print(env.observation_space.shape)  ->  (2,)\n",
    "\n",
    "buffer = deque(maxlen=20000)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes):\n",
    "  state, _ = env.reset()\n",
    "  terminate, truncate = False, False\n",
    "\n",
    "  for step in range(1, 201): #200 step to win\n",
    "\n",
    "    action = nnq_class.action(state)\n",
    "    next_state, reward, terminate, truncate, _ = env.step(action)\n",
    "\n",
    "    buffer.append((state, action, next_state, reward, terminate))\n",
    "    state = next_state\n",
    "\n",
    "    if step % 20 == 0 :\n",
    "      if len(buffer) > batch :\n",
    "          nnq_class.replay_buffer(buffer, batch)\n",
    "\n",
    "    if terminate or truncate:\n",
    "\n",
    "        reward_single_episode.append(step * -1)\n",
    "        average_reward = sum(reward_single_episode) / len(reward_single_episode)\n",
    "        episode_result.append(episode)\n",
    "        reward_result.append(average_reward)\n",
    "\n",
    "        print(f\"Episode: {episode} - Average Reward: {average_reward:.4f}   ----   epsilon(randomness): {nnq_class.epsilon:.4f} - learning rate: {nnq_class.learning_rate:.4f}\")\n",
    "        break\n",
    "\n",
    "  if episode % 200 == 0 :\n",
    "    nnq_class.net.save(f'./nn/nn-{episode}.h5')\n",
    "\n",
    "    plt.plot(episode_result, reward_result)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.ylim(-200, None)\n",
    "    plt.show()\n",
    "\n",
    "  if episode == 1200:\n",
    "    batch = 32\n",
    "  if episode == 2500:\n",
    "    batch = 16\n",
    "\n",
    "  if episode % 5 == 0 :\n",
    "    nnq_class.decresing_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbY4E1M7Z28N"
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "load_file = \"200\"\n",
    "average_reward, win_episode = [], 0\n",
    "learning_rate=0.01\n",
    "epsilon= 1.0\n",
    "episodes = 20\n",
    "\n",
    "nnq_class = nnq(env, learning_rate, epsilon)\n",
    "nnq_class.net = load_model(f'./nn_model/nn7.h5')\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    terminate, truncate, episode_reward = False, False, 0.0\n",
    "\n",
    "    while not terminate and not truncate:\n",
    "      action = nnq_class.best_action(state)\n",
    "\n",
    "      next_state, reward, terminate, truncate, _ = env.step(action)\n",
    "      episode_reward += reward\n",
    "      state = next_state\n",
    "\n",
    "      if next_state[0] >= 0.5:\n",
    "        win_episode += 1\n",
    "\n",
    "    average_reward.append(episode_reward)\n",
    "    print(f\"Episode: {episode} - Episode reward: {episode_reward:.2f}\")\n",
    "\n",
    "\n",
    "mean = sum(average_reward) / len(average_reward)\n",
    "accuracy = win_episode / episodes\n",
    "\n",
    "print(f\"\\n\\nAverage Reward: {mean:.2f}, Accuracy {accuracy:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
